the project : flight operations analytics

we will use airflow for automatization for workflow of the trasnformation and cleand of the data

architecture project :
data source Api >> apache airflow[raw data >> cleaned data >> business data ] >> dashboard in snowflake
for data source ap we will use skyog which provides real fliht data 
then data will be stored in rawin bronze layer 
Analyse flights operations 
objectifs :
la moyenne de vitesse de l'avion dans le pays qu'est fait le pplus de flights
l'analyse de flights in marocoo 
l'analyse le nombre de fois que l'avion qui en flight dans marroc 
then cleaned and stored in sliver layer  
then goas through some aggregations for making business data then store that in json files for bronze et sliver layer 
for business data we stored in csv
installation of airflow
docker compose cotient all information concern airflow and postgres
then .env for create compte for postgres and airflow
then
docker compose up -d 
docker compose logs airflow-init

dataset : 
context general :
ce dataset provient de opensky network est une plateforme open source qui collecte en temps reel les donnees ADS-B des avions du monde entier via un reseau mondial de recepteurs volontaires
il s'agit d'un vecteur d'etat par avion mis a jour toutes les quelques secondes
c'est l'une des sources le splus utilisees pour :
- suivi en temps reel des vols (cartes filghtrader like)
analyse de trafic aerien
recherche en aviation , securite , env n ml trajectoires
chaque ligne est un avion observe à un instant

les colonnees de dataset :

icao24:Identifiant unique 24 bits de l'avion (hex)
callsign : Indicatif radio du vol
origin_country : Pays d'immatriculation de l'avion
time_position : Date/heure de la dernière position connue
last_contact : Date/heure du dernier message reçu
longitude : Longitude (degrés)
latitude : Latitude (degrés)
baro_altitude : Altitude barométrique (mètres) ( la hauteur d'un avion deduite de la mesure de la presssion atmosphere ambiante via un altimetre
on_ground : avion ausol? (boolean)
velocity : Vitesse au sol (m/s)
true_track : Cap vrai (degrés, 0 = nord)
vertical_rate : Vitesse verticale (m/s)
sensors : Liste des récepteurs qui ont vu l'avion
geo_altitude : Altitude réelle (géométrique, mètres)
squawk : Code transpondeur (7700 = urgence, 7600 = panne radio…)
spi : Indicateur spécial activé ?
position_source : Source des coordonnées
category : Type / taille / catégorie de l'appareil
From dataset , we will pick up juste few columns:
icao24,origin_country ,velocity,baro_altitude

dossier :
scripts :
bronse script : 
Il télécharge en temps réel la position de tous les avions suivis dans le monde via l’API publique OpenSky Network, sauvegarde cette donnée brute sous forme de fichier JSON horodaté, et transmet le chemin du fichier aux tâches suivantes via XCom
ce fichier json stocker dans / opt/airflow/data/bronze pour l'utilsie pour l'etape sliver

sliver script :
Ce code prend les données brutes téléchargées depuis l’API OpenSky (fichier JSON créé par la tâche bronze) et les transforme en un fichier CSV propre et réduit, prêt pour les agrégations futures (gold layer).
C’est la couche silver :

Nettoyage minimal
Sélection des colonnes utiles
Format structuré (CSV)
Conservation uniquement des informations nécessaires pour l’analyse

le fichier csv stocker dans /opt/airflow/data/sliver pour le tache suivant gold 
dans fichier csn contient icao24: identifant unique de l'avion , origin_country : pays d'origin de l'avion , velocity : vitesse en m/s et on_ground: true sil'avion est au sol , false à sinon

gold script :
Ce code transforme des observations d’avions individuels (silver) en KPI par pays (gold), dernière étape avant le chargement dans la base de données analytique.

apres la creation d'un table des measures kpis pour l'analyse
on essyaer de connect airflow au snowflake pour transmettre le dataset a snowflake afin de cree un dashboard 
1- on essaye de cree dans snowflake un database nomme flights puis on cree un schema appele kpis puis cree un table flights_kpis 
2- apres on passse a airflow la partie connections remplir les informations cles pur connecte auu snowflake puis on cree un script pour etre lie les taches avec partie de connection a snowlfake pour transmettre les donnes a table flights kpis
load_to snowlfkae script : 
Ce code est la dernière étape (tâche de chargement/load) de ton pipeline Airflow de données de vols.
Il prend le fichier CSV gold (agrégations par pays : nombre de vols, vitesse moyenne, avions au sol) produit par la tâche précédente, et l’insère ou met à jour dans une table Snowflake nommée FLIGHTS_KPIS

apres il ya fichier flights_pipleine contient pipeline qui liee le s4 etapes dans le dosiser dags
C’est le fichier principal qui orchestre tout le flux de traitement des données de vols en temps réel, en suivant l’architecture médaille (bronze → silver → gold → load)

apres il y a partie de l'analyse par creation un dashbaord :

1- analyse top pays avec un nombre de flights grands
2- analyse top pays avec un nombre de avions dans sol
3- puis united states est le top pay avec un nombre de flighst , l'analyse lamoyenend e vitesse au tour de window start je vois que le vitesse moyenne stable au cours de temps  

